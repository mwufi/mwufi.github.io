<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style id="typography.js">html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}[hidden],template{display:none}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit;font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,input{overflow:visible}button,select{text-transform:none}[type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{font:112.5%/1.44 'Fira Sans',sans-serif;box-sizing:border-box;overflow-y:scroll;}*{box-sizing:inherit;}*:before{box-sizing:inherit;}*:after{box-sizing:inherit;}body{color:hsla(0,0%,0%,0.8);font-family:'Fira Sans',sans-serif;font-weight:400;word-wrap:break-word;font-kerning:normal;-moz-font-feature-settings:"kern", "liga", "clig", "calt";-ms-font-feature-settings:"kern", "liga", "clig", "calt";-webkit-font-feature-settings:"kern", "liga", "clig", "calt";font-feature-settings:"kern", "liga", "clig", "calt";}img{max-width:100%;margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}h1{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:2.15rem;line-height:1.1;}h2{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:1.58293rem;line-height:1.1;}h3{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:1.35824rem;line-height:1.1;}h4{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:1rem;line-height:1.1;}h5{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:0.85805rem;line-height:1.1;}h6{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;color:hsla(0,0%,0%,1);font-family:'Playfair Display',serif;font-weight:700;text-rendering:optimizeLegibility;font-size:0.79482rem;line-height:1.1;}hgroup{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}ul{margin-left:1.44rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;list-style-position:outside;list-style-image:none;}ol{margin-left:1.44rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;list-style-position:outside;list-style-image:none;}dl{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}dd{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}p{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}figure{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}pre{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;font-size:0.85rem;line-height:1.44rem;}table{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;font-size:1rem;line-height:1.44rem;border-collapse:collapse;width:100%;}fieldset{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}blockquote{margin-left:0;margin-right:1.44rem;margin-top:0;padding-bottom:0;padding-left:1.17rem;padding-right:0;padding-top:0;margin-bottom:1.08rem;font-size:1.16543rem;line-height:1.44rem;color:hsla(0,0%,0%,0.59);font-style:italic;border-left:0.27rem solid hsla(0,0%,0%,0.2);}form{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}noscript{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}iframe{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}hr{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:calc(1.08rem - 1px);background:hsla(0,0%,0%,0.2);border:none;height:1px;}address{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.08rem;}b{font-weight:700;}strong{font-weight:700;}dt{font-weight:700;}th{font-weight:700;}li{margin-bottom:calc(1.08rem / 2);}ol li{padding-left:0;}ul li{padding-left:0;}li > ol{margin-left:1.44rem;margin-bottom:calc(1.08rem / 2);margin-top:calc(1.08rem / 2);}li > ul{margin-left:1.44rem;margin-bottom:calc(1.08rem / 2);margin-top:calc(1.08rem / 2);}blockquote *:last-child{margin-bottom:0;}li *:last-child{margin-bottom:0;}p *:last-child{margin-bottom:0;}li > p{margin-bottom:calc(1.08rem / 2);}code{font-size:0.85rem;line-height:1.44rem;}kbd{font-size:0.85rem;line-height:1.44rem;}samp{font-size:0.85rem;line-height:1.44rem;}abbr{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}acronym{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}abbr[title]{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;text-decoration:none;}thead{text-align:left;}td,th{text-align:left;border-bottom:1px solid hsla(0,0%,0%,0.12);font-feature-settings:"tnum";-moz-font-feature-settings:"tnum";-ms-font-feature-settings:"tnum";-webkit-font-feature-settings:"tnum";padding-left:0.96rem;padding-right:0.96rem;padding-top:0.72rem;padding-bottom:calc(0.72rem - 1px);}th:first-child,td:first-child{padding-left:0;}th:last-child,td:last-child{padding-right:0;}a{color:#9f392b;}blockquote > :last-child{margin-bottom:0;}blockquote cite{font-size:1rem;line-height:1.44rem;color:hsla(0,0%,0%,0.8);font-weight:400;}blockquote cite:before{content:"â€” ";}@media only screen and (max-width:480px){blockquote{margin-left:-1.08rem;margin-right:0;padding-left:0.81rem;}}</style><style data-href="/styles.c47d77a4e7fe129b3e91.css">*{margin:0;padding:0;box-sizing:border-box}body,html{width:100%;min-height:100%}html{border-top:7px solid salmon;border-left:7px solid rgba(255,0,0,.05)}pre{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.canvas-module--canvas--3sdYz{position:fixed;left:0;width:100%;height:100%;border:2px solid rgba(255,0,255,.2);border-radius:3px}</style><meta name="generator" content="Gatsby 2.19.7"/><link href="//fonts.googleapis.com/css?family=Playfair+Display:700|Fira+Sans:400,400i,700,700i" rel="stylesheet" type="text/css"/><link rel="icon" href="/icons/icon-48x48.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="theme-color" content="#6b37bf"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=131e3c6692f5e33eadbadcdd3f62336f"/><title data-react-helmet="true">Neural image rendering | South by Zen</title><meta data-react-helmet="true" name="description" content="Novel view synthesis We want to construct novel views from one or more source images. Ie, given these images, can we move the camera? Youâ€¦"/><meta data-react-helmet="true" property="og:title" content="Neural image rendering"/><meta data-react-helmet="true" property="og:description" content="Novel view synthesis We want to construct novel views from one or more source images. Ie, given these images, can we move the camera? Youâ€¦"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:creator" content="zenriaaa"/><meta data-react-helmet="true" name="twitter:title" content="Neural image rendering"/><meta data-react-helmet="true" name="twitter:description" content="Novel view synthesis We want to construct novel views from one or more source images. Ie, given these images, can we move the camera? Youâ€¦"/><link as="script" rel="preload" href="/webpack-runtime-3753fb30b972f26bea86.js"/><link as="script" rel="preload" href="/styles-486c55ee1b6f9178387a.js"/><link as="script" rel="preload" href="/app-7783c9a49a31c06cd306.js"/><link as="script" rel="preload" href="/commons-59449bedb7e05b4fcc5a.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-ac570e066dc2ca5f7d8a.js"/><link as="fetch" rel="preload" href="/page-data/machine-learning/neural-image-rendering/page-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" role="group" id="gatsby-focus-wrapper"><style data-emotion-css="wgvobt">.css-wgvobt{width:100%;padding:14px;-webkit-transition:background 200ms ease;transition:background 200ms ease;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;}.css-wgvobt *{-webkit-text-decoration:none;text-decoration:none;}</style><header class="css-wgvobt"><style data-emotion-css="k1jnzr">.css-k1jnzr{width:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:space-around;-webkit-justify-content:space-around;-ms-flex-pack:space-around;justify-content:space-around;}.css-k1jnzr a{position:relative;padding:3px;-webkit-transition:all ease 0.2s;transition:all ease 0.2s;border-top:7px solid transparent;border-right:7px solid transparent;}.css-k1jnzr a:hover{border-top:7px solid salmon;border-right:7px solid rgba(255,0,0,0.05);}</style><div class="css-k1jnzr"><a href="/home">Home</a><a href="/research/">Research &amp; Talks</a><a href="/">Blog</a><a href="/about/">About</a></div></header><style data-emotion-css="1pygrj1">.css-1pygrj1{margin:0 auto;max-width:800px;padding:2.88rem;padding-top:2.16rem;}@media only screen and (max-width:600px){.css-1pygrj1{padding:1.44rem;}}</style><div class="css-1pygrj1"><style data-emotion-css="7wh13m">.css-7wh13m{display:block;width:100%;}</style><div class="css-7wh13m"><style data-emotion-css="rnei24">.css-rnei24{display:block;text-align:center;background-color:rgba(255,0,0,0.05);margin-bottom:3em;margin-left:2em;margin-right:2em;padding:2em;border-radius:7px;}@media only screen and (max-width:600px){.css-rnei24{margin-left:0;margin-right:0;padding:2.88rem;}}</style><h1 class="css-rnei24">Neural image rendering</h1></div><div><h1>Novel view synthesis</h1>
<p>We want to construct novel views from one or more source images. Ie, given these images, can we move the camera?</p>
<p>You have a couple of options:</p>
<ul>
<li>Do you want 3D geometry? You can start by </li>
<li>View synthesis from a single image using ground-truth depth or semantics</li>
<li>View synthesis from regular pictures. No depth, no semantics.</li>
<li>Purely imaginary view synthesis - Generative models work</li>
</ul>
<h2>1. Representing 3D structure</h2>
<p>One way of representing 3D structure is depth. Assuming there is no ground-truth depth available for your images, maybe you can guess depth somehow. Getting good depth estimates from a single image is an entire subfield by itself! ("single image depth estimation")</p>
<p>In recent work like SynSin, we learn this by creating <em>differentiable renderers</em>. The renderer incorporates 3D priors into the pipeline.</p>
<p>An interesting approach would be to see if we can do the same thing without this renderer. If so, we'll need to represent 3D structure in another way, in order to feed it to the second part</p>
<h2>2. Generating images</h2>
<p>This bit is more flexible :)</p>
<h1>References</h1>
<p><a href="link">SynSin</a></p>
<blockquote>
<p>The main idea of SynSin is that there's a differentiable renderer which allows efficient learning of image-to-point-cloud feature correspondences. Using a point cloud (via a depth network) also allows you to do explicit 3D scenes.</p>
</blockquote>
<p>Pretty cool!</p>
<p><a href="link">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (2018)</a></p>
<blockquote>
<p>Can generate images of KITTI and CityScapes!</p>
</blockquote>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 800px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/a2994a1f0bb1e6025281a29a51b44353/69476/instance.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 40.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB9klEQVQoz62Sz08SABTH/Re6sDbLWIC4wqCZBOWEzNSlCQiCArYOupW1cilCJOSqlZqXFpZzYbM8uJXyUwNaHto6sHWUEpzlzNyqeefEJ6A1/oHe9t3e2/e9z+G9V5bL5fhfUWCVFZLUlzUi0SViiWWiiRXCbyNFxVYTBKIBPq+vkdlIEwq/IRhaIhJf4UPyI8FoiEA4wOvQIslPySK0CHTfcaNtqaPV0oqoQoCmrpYbrgGuD1+j+rgUl8eNb+YpCvUxtG0NSMUH0WrVdNn1WO06NI1qHPe9JaBr5BaqehVtpguIBfu42Gfnrm+cfudVzKYmXrzyM+2fQXWmDktPFzVVhzFYDIzPPqO7r4cOQzOPZ6dKwOnnU8gUlQglQqoqypFUS6lpqKXZpMGoV+Of8xEML6JUy6kqeEfEHD0hR92koanzPN02HWNPHpWALxfmsPaa0Vt1NKgUyITlVB7aj0Yp4rRcwIMJL/H3MS5dtmLL97WfO8VJmQSZ6AAtZ5U01isY8jhKwMRqHNeok2GvE8ftQdyem/ndmDEadVhsHcwvzPMuf6Ah9wCee26cI4NMTI5ypb+Xdl0rnVYTDyfH/gL/vc3e3h6bm5tkMhnSeWUyG3z9tsXPX7+L/lY+LyidTpNKpfixu8v3nR3W8/X29nZxLpvN8ge2tr0QRgNzFQAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="instance map"
        title="instance map"
        src="/static/a2994a1f0bb1e6025281a29a51b44353/5a190/instance.png"
        srcset="/static/a2994a1f0bb1e6025281a29a51b44353/772e8/instance.png 200w,
/static/a2994a1f0bb1e6025281a29a51b44353/e17e5/instance.png 400w,
/static/a2994a1f0bb1e6025281a29a51b44353/5a190/instance.png 800w,
/static/a2994a1f0bb1e6025281a29a51b44353/69476/instance.png 926w"
        sizes="(max-width: 800px) 100vw, 800px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
      />
  </a>
    </span></p>
<p><a href="link">Monocular Neural Image based Rendering with Continuous View Control</a></p>
<blockquote>
<p>In this paper, we propose a novel learning pipeline that de- termines the output pixels directly from the source color but forces the network to implicitly reason about the underlying geometry</p>
</blockquote>
<p>To do this, we do two things:</p>
<ul>
<li>Make an encoder which can model viewpoint changes: they do this by using a transforming autoencoder. By explicitly rotating (e.g. multiplying by a matrix) the latent vector during training, it can be taught to be interpretable in that way. So during test time, we can feed it a test vector and it'll produce a depth map of a transformed perspective.</li>
<li>Use the target depth map to render the image! The hard part is pretty done, now we just have to use the result to render an image. Basically pick which colors go to which pixels, depending on this super useful depth we get.</li>
</ul>
<p>A lot of these works cite [MaximTatarchenko,AlexeyDosovitskiy,andThomasBrox], who did this thing in 2016.</p>
<p><strong>KITTI</strong>
In total there are 18560 images for training and 4641 images for testing. We construct training pairs by randomly selecting target view among 10 nearest frames of source view. The relative transformation is obtained from the global camera poses</p>
<h1>Birds</h1>
<p><a href="link">Self-supervised Single-view 3D Reconstruction via Semantic Consistency</a>
<span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 800px;"
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 59.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAACiUlEQVQoz6WSXU/TYBTH9/38BMZ4Y4xfwOiViRdqML5xIWoQjBFfUFCn4lDUoSLgOqLgYGCAwba6td3atetYu3Zrfz57ph/AeC6a/3Ne/ud/Tk9CN+uE3ZCw06GiloniiJbjUTMaWJaFWtUwDA2zXsduOrgHnsA1ajUdTdPxOx6ObdFoNOhbYuJRCmXnMWW7iep2pPP1i2vU6muiESzMZmgddKU/83ELo1LFsgM216sQd/j0oYBu+CIaEccxifGzpzl17TjlpkUy78jC63dHuDp8RGJl8j6zKYXVnMrj4VFR1+BnwWT09isZ/5r+zrmhZ0TRoGliITvCXHJcOEI8L8ALQxnQtSZB6Eps6C7FoiZy5JNuGNCw2/w1rSpW0fIHhC9Tt9BLS8Tisd/u4veg1frJu5nLRAKreo6SmpbJaukTvmdJvF9RxIiCrJ6jqGWkT4588sxhlr9M0PAgvWMSiiQlM83Q9QsyaW5hio2tlMQbm09x3SrdXkR6cYa+irKeJ/n+IUEnGCgMxZiWZYrgn3mk1n+3vjpJ+DT5kNy3LCXXpyrW1/S6pN+NUiw945exx/LKZ/RfBWrVMkvKPE6rie3YbO1s0V9qPqeQ3ViRZP2TSzy5c5E3r6Yp2R45s43lR7z8MMXc+0vUjV2+Lc7yY2WBzR9ZMvNJOn4b06yxu50X2Gc+NUm5kBuoFA0SphPjHPj8r8V/vomJW0e5N3yMfdtntd1DE+RK9jwjVw9RLazjNV9Q270hC8obE7Sdopg0oLw9SS+KsdU31Pdu0usOTiyxNnuC7MwYRjNAUW3coMtK5i1XxsZwvRZmcZpKfnpwKqvPaVkVcU4+22sPZJP6voK6fkfsb/CXfwNdaW0ccs+OGQAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="birds"
        title="birds"
        src="/static/19a9e1673124f970cf08c11bc789b756/5a190/birds.png"
        srcset="/static/19a9e1673124f970cf08c11bc789b756/772e8/birds.png 200w,
/static/19a9e1673124f970cf08c11bc789b756/e17e5/birds.png 400w,
/static/19a9e1673124f970cf08c11bc789b756/5a190/birds.png 800w,
/static/19a9e1673124f970cf08c11bc789b756/c1b63/birds.png 1200w,
/static/19a9e1673124f970cf08c11bc789b756/098c1/birds.png 1578w"
        sizes="(max-width: 800px) 100vw, 800px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
      />
    </span></p>
<h1>Predicting depth first</h1>
<p>Another way to do 3D reconstruction is to predict the depth from input images. This is a pretty common approach.</p>
<p><a href="link">3D Ken Burns</a></p></div></div></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/machine-learning/neural-image-rendering/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-7783c9a49a31c06cd306.js"],"component---node-modules-gatsby-plugin-offline-app-shell-js":["/component---node-modules-gatsby-plugin-offline-app-shell-js-cf2fd0f92da7499f18b1.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-ac570e066dc2ca5f7d8a.js"],"component---src-pages-about-js":["/component---src-pages-about-js-cf546683de5f0a312b27.js"],"component---src-pages-canvas-js":["/component---src-pages-canvas-js-21bd4b0a1fe65733ce72.js"],"component---src-pages-dev-js":["/component---src-pages-dev-js-5ce134c64317b1d46d8e.js"],"component---src-pages-home-js":["/component---src-pages-home-js-72fafd1d4fab16780632.js"],"component---src-pages-index-js":["/component---src-pages-index-js-7cedab1516a3cacd903c.js"],"component---src-pages-research-js":["/component---src-pages-research-js-aa4123adc97fdd3b948c.js"],"component---src-pages-typography-js":["/component---src-pages-typography-js-8846227301753f86489e.js"]};/*]]>*/</script><script src="/component---src-templates-blog-post-js-ac570e066dc2ca5f7d8a.js" async=""></script><script src="/commons-59449bedb7e05b4fcc5a.js" async=""></script><script src="/app-7783c9a49a31c06cd306.js" async=""></script><script src="/styles-486c55ee1b6f9178387a.js" async=""></script><script src="/webpack-runtime-3753fb30b972f26bea86.js" async=""></script></body></html>
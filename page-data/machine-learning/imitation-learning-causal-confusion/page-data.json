{"componentChunkName":"component---src-templates-blog-post-js","path":"/machine-learning/imitation-learning-causal-confusion/","result":{"data":{"markdownRemark":{"id":"c8555735-9342-51a5-94ea-d21dbaa04a37","html":"<p>In imitation learning, an expert demonstrates how to perform a task (e.g., driving a car, filling a cup, playing a game) for the benefit of an agent.</p>\n<p>In each demo, the agent has access both to its n-dim. state observations at each time t, X t = [X1t , X2t , . . . Xnt ] (e.g., a video feed from a camera), and to the expert’s action At.</p>\n<p>Behavioral cloning approaches learn a mapping π from Xt to At using all (Xt, At) tuples from the demonstrations.</p>\n<p>Seems easy enough, right?</p>\n<h1>The phenomenon of Causal identification</h1>\n<h3>Confounders</h3>\n<p>A confounder Z t = [X t−1 , At−1 ] influences each state variable in X t , so that some nuisance variables may still be correlated with At among (X t , At ) pairs from demonstrations</p>\n<p>How prevalent are confounders in real life problems?</p>\n<p>Applying this formalism to our imitation learning setting, any distributional shift in the state Xt may be modeled by intervening on Xt, so that correctly modeling the “interventional query” p(At|do(Xt)) is sufficient for robustness to distributional shifts. Now, we may formalize the intuition that only a policy relying solely on true causes can robustly model the mapping from states to optimal/expert actions under distributional shift.</p>\n<h3>Experiments</h3>\n<p>For each task, we study imitation learning in\ntwo scenarios. In scenario A (henceforth called\n\"CONFOUNDED\"), the policy sees the augmented ob-\nservation vector, including the previous action.</p>\n<p>In scenario B\n(\"ORIGINAL\"), the previous action variable is replaced with random noise for low-dimensional ob- servations. </p>\n<p>ORIGINAL produces rewards tending towards expert performance as the size of the imitation dataset increases. CONFOUNDED either requires many more demonstrations to reach equivalent performance, or fails completely to do so.</p>\n<p>This is weird, no? By simply giving the model access to additional information (the previous action), we've totally messed up its ability to learn!</p>\n<h3>Other stories</h3>\n<p>Let's detour to Wang et al [56]</p>\n<p>We draw the reader’s attention to particularly telling results from Wang et al. [56] for learning to drive in near-photorealistic GTA-V [24] envi- ronments, using behavior cloning with DAgger- inspired expert perturbation. Imitation learn- ing policies are trained using overhead image observations with and without “history” infor- mation (HISTORY and NO-HISTORY) about the ego-position trajectory of the car in the past.</p>\n<p>nd once again, like in our tests above, HISTORY has better performance on held-out demonstration data, but much worse performance when actually deployed</p>\n<h1>Resolving Causal Misidentification</h1>","frontmatter":{"title":"Causal Confusion, IL, and Learning Models of the World"},"excerpt":"In imitation learning, an expert demonstrates how to perform a task (e.g., driving a car, filling a cup, playing a game) for the benefit of…"}},"pageContext":{"slug":"/machine-learning/imitation-learning-causal-confusion/"}}}